# Candle Model Loading - Implementation Summary

## Overview

CheckStream now supports loading and running ML models using **Candle**, Hugging Face's minimalist ML framework for Rust. This enables native Rust inference for Tier B and Tier C classifiers without Python dependencies.

## What Was Implemented

### 1. Model Configuration System
- **ModelConfig**: Builder pattern for configuring model loading
- **ModelSource**: Support for both local files and Hugging Face Hub
- **DeviceType**: CPU, CUDA (NVIDIA GPU), Metal (Apple Silicon)
- **ModelFormat**: SafeTensors (recommended) and PyTorch formats

### 2. Model Loading Infrastructure
- **LoadedModel**: Container for loaded weights, tokenizer, and metadata
- **VarBuilder integration**: Direct integration with Candle's weight loading
- **Automatic tokenizer loading**: From local files or Hugging Face
- **Model metadata extraction**: Name, version, dimensions, etc.

### 3. Model Registry
- **ModelRegistry**: Centralized management of multiple models
- **Load and register**: One-step model loading and registration
- **Model lookup**: Fast retrieval by name
- **Model discovery**: Auto-discover models in directories

### 4. Hugging Face Integration
- **Auto-download**: Automatically download models from HF Hub
- **Caching**: Models cached locally after first download
- **Tokenizer download**: Automatic tokenizer retrieval
- **Revision support**: Specify git revisions/branches

## Key Features

### Supported Model Sources

**Local Files:**
```rust
let config = ModelConfig::from_local("./models/model.safetensors");
```

**Hugging Face Hub:**
```rust
let config = ModelConfig::from_hf("unitary/toxic-bert", "model.safetensors");
```

### Device Support

**CPU (always available):**
```rust
.with_device(DeviceType::Cpu)
```

**NVIDIA GPU:**
```rust
.with_device(DeviceType::Cuda(0))  // GPU index
```

**Apple Silicon:**
```rust
.with_device(DeviceType::Metal(0))
```

### Model Formats

**SafeTensors (recommended):**
- Fast loading
- Memory safe
- `.safetensors` extension

**PyTorch:**
- Standard format
- `.pt`, `.pth`, `.bin` extensions

### Quantization Support
```rust
.with_quantization(true)  // Enable for faster inference
```

## Usage Examples

### Basic Model Loading

```rust
use checkstream_classifiers::{LoadedModel, ModelConfig, DeviceType};

// Load from Hugging Face
let config = ModelConfig::from_hf("unitary/toxic-bert", "model.safetensors")
    .with_device(DeviceType::Cpu)
    .with_quantization(true);

let model = LoadedModel::load(config)?;
```

### Using Model Registry

```rust
use checkstream_classifiers::ModelRegistry;

let mut registry = ModelRegistry::new();

// Load multiple models
registry.load_and_register(
    "toxicity",
    ModelConfig::from_hf("unitary/toxic-bert", "model.safetensors")
)?;

registry.load_and_register(
    "sentiment",
    ModelConfig::from_local("./models/sentiment.safetensors")
)?;

// Retrieve models
let toxicity_model = registry.get("toxicity").unwrap();
```

### Building a Classifier

```rust
use candle_transformers::models::bert::{BertModel, Config};

pub struct ToxicityClassifier {
    model: Arc<LoadedModel>,
    bert: BertModel,
}

impl ToxicityClassifier {
    pub fn new() -> Result<Self> {
        let config = ModelConfig::from_hf("unitary/toxic-bert", "model.safetensors");
        let loaded = LoadedModel::load(config)?;

        // Build BERT model from loaded weights
        let bert_config = Config::default();
        let vb = loaded.var_builder();
        let bert = BertModel::load(vb, &bert_config)?;

        Ok(Self {
            model: Arc::new(loaded),
            bert,
        })
    }
}
```

## File Structure

```
crates/checkstream-classifiers/src/
â”œâ”€â”€ model_loader.rs       # Model loading infrastructure (450+ lines)
â”œâ”€â”€ classifier.rs         # Classifier trait
â”œâ”€â”€ toxicity.rs          # Toxicity classifier (to be updated with Candle)
â”œâ”€â”€ pii.rs               # PII classifier (regex-based)
â””â”€â”€ patterns.rs          # Pattern matching classifier
```

## Dependencies Added

```toml
[workspace.dependencies]
candle-core = "0.8"          # Core Candle functionality
candle-nn = "0.8"            # Neural network layers
candle-transformers = "0.8"  # Transformer models (BERT, etc.)
tokenizers = "0.20"          # HuggingFace tokenizers
hf-hub = "0.3"              # HuggingFace Hub API
num_cpus = "1.16"           # CPU detection
```

## Performance Characteristics

### Model Loading Time
- **First load** (with HF download): Depends on model size and network
- **Cached load** (SafeTensors): ~100-500ms for BERT-base
- **Cached load** (PyTorch): ~200-800ms for BERT-base

### Inference Latency Targets
- **Tier B** (<5ms): Distilled models, quantized inference
- **Tier C** (<10ms): Full-size models, optimized execution

### Memory Usage
- **BERT-base**: ~440MB (unquantized)
- **DistilBERT**: ~260MB (unquantized)
- **With quantization**: 30-50% reduction

## Recommended Models

### Toxicity (Tier B)
- `unitary/toxic-bert` - BERT fine-tuned for toxicity
- `martin-ha/toxic-comment-model` - Distilled version
- Custom distilled models (<5ms inference)

### Sentiment (Tier B)
- `distilbert-base-uncased-finetuned-sst-2-english`
- `cardiffnlp/twitter-roberta-base-sentiment`

### Prompt Injection (Tier B)
- `deepset/deberta-v3-base-injection`
- Custom fine-tuned models

### Text Classification (Tier C)
- `bert-base-uncased` for custom fine-tuning
- `roberta-base` for more complex classification

## Integration with CheckStream

### Configuration
Models can be specified in `config.yaml`:
```yaml
models:
  toxicity:
    source: huggingface
    repo_id: unitary/toxic-bert
    filename: model.safetensors
    device: cpu
    quantize: true

  sentiment:
    source: local
    path: ./models/sentiment.safetensors
    device: cpu
```

### At Startup
```rust
// Initialize model registry
let mut registry = ModelRegistry::new();

// Load models from config
for (name, model_config) in config.models {
    registry.load_and_register(name, model_config)?;
}

// Pass to classifiers
let toxicity = ToxicityClassifier::from_registry(&registry, "toxicity")?;
```

## Testing

```bash
# Check compilation
cargo check --package checkstream-classifiers

# Run tests
cargo test --package checkstream-classifiers

# Run with model loading (requires actual model files)
cargo test --package checkstream-classifiers --features integration-tests
```

## Next Steps

1. **Implement Tier B Toxicity Classifier**
   - Replace placeholder with Candle-based implementation
   - Use loaded models for actual inference
   - Benchmark against <5ms target

2. **Add More Classifiers**
   - Prompt injection detection
   - Financial advice detection
   - Readability scoring

3. **Optimize Inference**
   - Benchmark different model sizes
   - Test quantization impact
   - GPU acceleration testing

4. **Create Model Zoo**
   - Pre-trained models for common use cases
   - Conversion scripts (PyTorch â†’ SafeTensors)
   - Performance benchmarks

## Troubleshooting

### Compilation Issues
```bash
# Update Cargo.lock
cargo update

# Clean and rebuild
cargo clean
cargo build --package checkstream-classifiers
```

### Model Download Issues
- Check internet connection
- Verify HuggingFace Hub access
- Check disk space for model cache

### Performance Issues
- Enable quantization
- Use smaller/distilled models
- Consider GPU acceleration
- Profile with `cargo flamegraph`

## Documentation

- **Detailed Guide**: [docs/model-loading.md](docs/model-loading.md)
- **Candle Documentation**: https://github.com/huggingface/candle
- **Candle Examples**: https://github.com/huggingface/candle/tree/main/candle-examples

## API Reference

### ModelConfig
```rust
pub struct ModelConfig {
    pub source: ModelSource,
    pub tokenizer_path: Option<PathBuf>,
    pub device: DeviceType,
    pub format: ModelFormat,
    pub quantize: bool,
}

impl ModelConfig {
    pub fn from_local(path: impl Into<PathBuf>) -> Self;
    pub fn from_hf(repo_id: impl Into<String>, filename: impl Into<String>) -> Self;
    pub fn with_tokenizer(self, path: impl Into<PathBuf>) -> Self;
    pub fn with_device(self, device: DeviceType) -> Self;
    pub fn with_quantization(self, enable: bool) -> Self;
    pub fn with_format(self, format: ModelFormat) -> Self;
    pub fn with_revision(self, revision: impl Into<String>) -> Self;
}
```

### LoadedModel
```rust
pub struct LoadedModel {
    // Internal fields
}

impl LoadedModel {
    pub fn load(config: ModelConfig) -> Result<Self>;
    pub fn var_builder(&self) -> &VarBuilder<'static>;
    pub fn device(&self) -> &Device;
    pub fn tokenizer(&self) -> Option<&Arc<Tokenizer>>;
    pub fn metadata(&self) -> &ModelMetadata;
    pub fn has_tokenizer(&self) -> bool;
    pub fn weights_path(&self) -> &Path;
}
```

### ModelRegistry
```rust
pub struct ModelRegistry {
    // Internal fields
}

impl ModelRegistry {
    pub fn new() -> Self;
    pub fn register(&mut self, name: impl Into<String>, model: LoadedModel);
    pub fn get(&self, name: &str) -> Option<Arc<LoadedModel>>;
    pub fn load_and_register(&mut self, name: impl Into<String>, config: ModelConfig) -> Result<()>;
    pub fn has_model(&self, name: &str) -> bool;
    pub fn model_names(&self) -> Vec<String>;
    pub fn clear(&mut self);
}
```

## Build Status

âœ… **Compilation**: Success
âœ… **Dependencies**: Candle 0.8 + tokenizers 0.20
âœ… **Tests**: Model config tests passing
ðŸ”„ **Integration**: Ready for classifier implementation

## Summary

Candle model loading is now fully integrated into CheckStream, providing a robust foundation for implementing production ML classifiers in Tier B and Tier C. The system supports both local and Hugging Face models, multiple devices (CPU/GPU), and includes comprehensive configuration and registry management.

**Next**: Implement actual Tier B/C classifiers using the loaded models!
